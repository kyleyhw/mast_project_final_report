\documentclass[floats,floatfix,showpacs,amssymb,prd,superscriptaddress,nofootinbib]{revtex4-2} % documentation at https://journals.aps.org/revtex/revtex-faq#u2
\bibliographystyle{apsrev}

% \documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xparse}
\usepackage[backend=biber]{biblatex}
\addbibresource{references.bib}
\usepackage{minted}
\usepackage{xcolor}
\definecolor{LightGray}{gray}{0.9}
\usepackage[left=2.54cm,right=2.54cm,top=2.54cm,bottom=2.54cm]{geometry}
\font\titlefont=cmr12 at 16pt
% inserting cover sheet: https://tex.stackexchange.com/questions/438775/how-to-insert-a-pdf-page-as-a-front-cover

% \newcommand{\PL}[1]{\textsf{\color{green!80!black}{\textsuperscript{PL}#1}}}
\newcommand{\code}{\texttt}
\newcommand{\red}{\textcolor{red}}

\setlength{\parindent}{20pt}
\renewcommand{\baselinestretch}{1.25}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\columnwidth]{images/Kyle_BLvsCR.png}
%     \caption{Recovered binary neutron star tidal parameters with and without binary Love relations, as compared to the common radius approximation.}
%     \label{fig:BLvsCR}
% \end{figure}

\begin{document}

\title{{\titlefont Effects of variable resolution and cosmological parameters
\\on the hydrogen 21cm cosmic dawn signal}\\{\small Supervised by Prof. Anastasia Fialkov and Jiten Dhandha}}
% project title : Impact of structure formation and cosmology on the hydrogen 21-cm signal from cosmic dawn
\date{\today}
\author{Kyle Wong}
\affiliation{Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge, CB3 0HA, UK}

% \begin{abstract}
% Summarize the problem we are solving and our main findings.
% \end{abstract}

\maketitle
\section{Introduction}
\subsection{21cm Cosmology}
Understanding the formation and evolution of cosmic structure remains one of the central goals of modern cosmology. While observations of the cosmic microwave background (CMB) and large-scale structure surveys have provided invaluable insights into the early and late-time universe, there exists a significant observational gap between the release of the CMB ($\sim$380,000 years after the Big Bang) and the emergence of the first luminous structures several hundred million years later. This intermediate period, encompassing the so-called Dark Ages, Cosmic Dawn, and the Epoch of Reionization (EoR), holds crucial information about the universe's thermal and ionization history, the formation of the first stars and galaxies, and the onset of feedback processes.

21cm cosmology offers a unique and powerful tool to probe this otherwise inaccessible era. The signal arises from the hyperfine transition of neutral hydrogen (HI), which occurs when the relative spin orientation of the proton and electron flips from parallel to antiparallel, emitting or absorbing a photon with a rest-frame wavelength of 21 centimeters (corresponding to 1.42 GHz). Because neutral hydrogen was the most abundant element in the early universe, the 21cm line provides a pervasive and potentially highly informative tracer of matter distribution over cosmic time.

As the universe expands, the 21cm signal is redshifted, allowing observations at different frequencies to correspond to different epochs. By mapping the sky across frequency channels, it is possible to construct a three-dimensional tomographic view of the intergalactic medium (IGM). This makes the 21cm line a particularly sensitive probe for the thermal history of the IGM, the timing and topology of reionization, the formation of the first stars and black holes, and potentially, physics beyond the standard cosmological model, such as dark matter interactions or exotic energy injection.

The brightness temperature of the 21cm signal, measured relative to the CMB, depends on the spin temperature of hydrogen, the neutral fraction, and the local density field. The differential brightness temperature can be written as

\begin{equation}
    \delta T_b (\nu) = 27 \ x_{HI} \ (1 + \delta_b) \left(1 - \frac{T_\gamma}{T_s} \right) \left(\frac{1 + z}{10} \frac{0.15}{\Omega_m h^2} \right)^{1/2} \left( \frac{\Omega_b h^2}{0.023} \right) \text{mK}
\end{equation}

\noindent where $x_{HI}$ is the neutral hydrogen fraction, $\delta_b$ is the baryon overdensity, $T_s$ is the spin temperature, $T_\gamma$ is the CMB temperature at redshift $z$, and $\Omega_m$, $\Omega_b$ are the matter and baryon density parameters respectively.

Detecting this signal presents substantial technical challenges. The cosmological 21cm signal is typically five orders of magnitude fainter than galactic and extragalactic foregrounds, including synchrotron emission from our Galaxy. Additionally, instrumental systematics, ionospheric effects, and radio frequency interference (RFI) must be mitigated with extreme precision.

Despite these obstacles, a growing number of dedicated low-frequency radio interferometers—such as LOFAR, MWA, HERA, and the upcoming Square Kilometre Array (SKA)—are designed to detect and characterize the 21cm signal from the early universe. These instruments aim to measure the power spectrum of 21cm fluctuations, and eventually, perform direct imaging of the neutral IGM.

21cm cosmology is poised to become a cornerstone of observational cosmology, potentially offering a detailed timeline of the universe's first billion years and enabling precision tests of fundamental physics in a previously uncharted epoch.

\section{21cmSPACE}
For any physical system, simulations are an invaluable asset for a multitude of reasons, including generating mock data from mathematical theory, which can then used for pipeline development and validation, as well as for foreground and instrumental modeling, which can then be used for experimental forecasting to inform instrument design. 

However, evolving the early universe on a machine is no easy task. Therefore, there does not exist one optimal way to simulate the period of time between recombination and reionization, but rather a multitude of different methods each with their own advantages and disadvantages. These methods lie on a spectrum with a trade off between accuracy and runtime. On one end lies numerical simulations, which hold accuracy as its foremost priority. This is achieved through explicit evolution of structure formation using hydrodynamic theory \red{cite thomas?}, which can either be freshly written with the intent of simulating the 21-cm signal from nativity, or taken from a generic library upon which processes such as radiative transfer and chemical evolution can then be attached for the specific purpose of evolving the 21-cm observables. This approach, due to its theoretical ability to include a comprehensive suite of physical effects, advertises the best possible control over the processes at each evolution step. However, numerical simulations come with the heavy downside of computational cost. \red{example?} While it is true that computer performance is exponentially increasing alongside decreasing costs, the need and expectation for improvements in simulation accuracy, size, and resolution have continued to render purely numerical simulations too expensive both in computational power and time to perform large-scale explorations of parameter spaces.

On the other end of the spectrum are analytical simulations, which, in contrast to numerical simulations, do not explicitly evolve spatial volumes through time. Instead, they solve mathematical equations, which, using the plethora of numerical solution libraries available, can take less than a second using commercial hardware \red{cite?}. However, this comes at the heavy trade-off of losing significant physical detail, with analytical equations having only the capability to model fields through averaged quantities, requiring significant approximations. Still, these calculations are of incredible value since these approximations are often subdominant compared to the inherently large observational error of the field. Despite their practical merit, though, the shortcomings of analytical simulations with regards to the lack of explicit evolution often prohibits output of full 21-cm signal maps or 21-cm power spectra.

Through the advantages and disadvantages of both numerical and analytical simulations, it is hopefully clear for the need of a third class: semi-numerical simulations. As the name suggests, semi-numerical simulations numerically resolve and evolve a spatial expanse, but rather than incorporating full hydrodynamic calculations of all processes at every step and every point in space, semi-numerical simulations invoke analytic calculations to deal with approximated quantities. Semi-numerical simulations therefore offer a compromise between accuracy and low computational cost. It is to this class of semi-numerical simulations that 21cmSPACE belongs.

%% much of the following section generated by GPT, summarizing and rephrasing gessey-jones chapter 2:

\subsection{Design principles}
21cmSPACE (21-cm Semi-numerical Predictions Across Cosmic Epochs) evolves large-scale structure using analytic or perturbative solutions, while key astrophysical processes are included via parametric, sub-grid models and numerical integration where necessary. The core aim of 21cmSPACE is to propagate the 21-cm brightness temperature field forward in time, in order to compute observable quantities such as the global (sky-averaged) 21-cm signal and its power spectrum. To achieve this, the code self-consistently tracks the evolution of all relevant fields -- e.g., the hydrogen spin temperature $T_S$, the background radiation temperature $T_\gamma$, the hydrogen neutral fraction $x_{\text{HI}}$, as well as derived quantities like star formation rates and radiation intensities.

By design, 21cmSPACE divides the problem according to scale: large-scale intergalactic fields (such as density, velocity, radiation backgrounds) are evolved on a coarsely resolved simulation grid, while small-scale phenomena (halo collapse, star formation, feedback) are handled by sub-grid prescriptions. \red{include illustration as in thomas thesis?} THis separation of scales is the key to computational efficiency, without needing to sacrifice essential physical processes.

Several design philosophies underlie 21cmSPACE. First, it emphasises flexibility in exploring astrophysical scenarios -- a wide array of input parameters \red{include table like in thomas thesis?} control star formation efficiencies, feedback strengths, spectral emissivities, etc., enabling the user to test different models of early-Universe astrophysics. For example, the efficiencies of Pop II  and Pop III star formation ($f_{\textasteriskcentered,II}$, $f_{\textasteriskcentered,III}$) and the delay time between Pop III and Pop II episodes ($t_{\text{delay}}$) are all tunable inputs. Likewise, the relative X-ray luminosity of high-$z$ X-ray binaries ($f_X$) and any additional radio background strength ($f_r$) can be specified. Second, the code is structured for performance: any components of the calculation that do not depend on the specific astrophysical parameters are precomputed once and stored for reuse. This includes, for instance, cosmological tables, linear perturbation growth factors, and radiation window functions (used for fast radiative transfer, discussed later). Third, 21cmSPACE assumes a fixed cosmological model during a run (by default the Planck 2013 $\Lambda$CDM parameters) \red{and is optimized under that assumption. In principle the cosmology can be changed by updating internal lookup tables and regenerating precomputed grids, but the code is primarily designed to vary astrophysical inputs rather than cosmological ones, since uncertainties in high-$z$ astrophysics are much larger than current cosmology uncertainties}. Overall, the architecture of 21cmSPACE prioritizes physical fidelity (by including all major known 21-cm relevant processes) while maintaining speed through analytical treatments and precomputation. This makes it well-suited to produce rapid predictions of 21-cm observables across cosmic dawn and reionization for a range of scenarios.

\subsection{Temporal, spatial, and cosmological framework}

Before any simulation, the time, space, and cosmology domains must be configured. 21cmSPACE uses a fixed temporal stepping scheme in redshift space, spanning redshift $z\approx50$ down to $z=6$. The simulation begins at $z=50$, a time when structure formation is in its infancy and linear theory is valid, and ends at $z=6$, by which point reionization is essentially complete and the cosmological 21-cm signal is vanishing. The time steps are not uniform: from $z=50$ to $15$ the code advances in unit redshift increments $\Delta z=1$, and then switches to finer steps of $\Delta z=0.1$ from $z=15$ to $6$. This refinement at lower redshifts is to resolve rapid changes during reionization, while larger steps at high redshift suffice when the evolution is slow. Starting at $z=50$ is deliberate – it ensures the initial conditions can be taken directly from linear perturbation theory (with negligible nonlinear structure or star formation by that point). Accordingly, 21cmSPACE initialises each simulation with density, velocity, temperature, and ionisation fields drawn from standard linear codes: matter overdensity and baryon–dark matter relative velocity fields are generated from the linear power spectra (e.g., using CAMB outputs), and initial gas temperature and ionised fraction come from primordial recombination calculations (e.g. RECFAST). These initial conditions at $z=50$ establish the proper starting state of the intergalactic medium (IGM) before the first stars.

In space, 21cmSPACE uses a three-dimensional Cartesian grid of cells to discretise the simulation volume. The cell size and number are chosen to balance two competing needs: capturing a large volume for statistical representativeness, and maintaining sufficient resolution to model the relevant physical scales. Throughout the project work, a grid of $128^3$ cubical cells was used, with each cell being 3 comoving Mpc (cMpc) on each side. This yields a simulation box of side length $128\times3 = 384$ cMpc and volume $(384~\text{cMpc})^3$. Such a volume is large enough to provide meaningful predictions for global signals and power spectra (covering wavenumbers $k \sim 0.05$–1.0 cMpc$^{-1}$, matching the range probed by current interferometers like HERA). At the same time, a $3~\text{cMpc}$ cell size is fine enough to resolve features like the characteristic scale of baryon–dark matter streaming motions ($\sim 10~\text{cMpc}$ coherence length), and yet coarse enough that individual cell regions remain roughly in the linear regime until $z\sim6$. (In fact, if the cells were much smaller, dense regions would collapse and deviate from linear evolution earlier, violating assumptions in the code’s analytic updates.) The chosen 3 cMpc resolution thus ensures the sub-grid astrophysical models (for star formation, radiative sources, etc.) remain valid on the cell scale up to the end of the simulation. It is worth noting that 21cmSPACE is not limited to $128^3$ – as of 2023, the code supports arbitrary grid sizes $N_{\rm cell}$, allowing much larger volumes (e.g. to cover the full SKA-Low field of view) if computational resources permit.

\red{As mentioned, the simulation assumes a $\Lambda$CDM cosmology (Planck 2013 parameters by default) for computing expansion history and linear perturbations. Within each time step, cosmic expansion is accounted for in all equations (e.g. redshifting of radiation, Hubble cooling of gas). The code updates the background CMB temperature $T_\gamma(z)$ according to $T_\gamma = 2.725(1+z)$ K (the CMB is taken as the default radio background) unless an additional radio background is specified. While alternative cosmologies could be input by manually editing the cosmological parameter table and recomputing certain grids, the architecture is optimized for a fixed cosmology, leveraging the fact that astrophysical unknowns dominate over cosmological ones for the 21-cm signal. In summary, the simulation grid and timeline are configured to reliably capture the full 21-cm relevant epoch (~50 $\gt$ z $\gt$ 6) with adequate resolution and physical initial conditions, providing the stage on which the astrophysical simulation unfolds.}

\subsection{Main Simulation Loop and Sequence of Operations}

Once initialized, 21cmSPACE enters its main simulation loop, iterating over each time step (redshift decrement) from $z=50$ down to $z=6$. At each step, the code updates all relevant fields in a prescribed sequence. The following outlines the major steps executed in one simulation cycle (Gessey-Jones 37–38):

\begin{enumerate}
    \item Update large-scale cosmological fields: The cosmological density fluctuation $\delta(\mathbf{x})$ in each cell and the baryon–dark matter relative velocity $v_{\text{bc}}(\mathbf{x})$ are advanced from the previous redshift to the new redshift. While density fluctuations eventually grow non-linear in overdense cells, in practice 21cmSPACE applies linear growth (scaling by the linear growth factor) at each small redshift step, which is a good approximation on the chosen grid scale until late times. Similarly, the streaming velocity decays with time according to linear theory and is updated analytically (Gessey-Jones 29, 37). These updates account for the cosmological expansion and structure growth between time steps.

    \item Calculate halo abundance in each cell: Using the updated density (and residual streaming velocity), the code computes the halo mass function locally in each cell. 21cmSPACE employs an analytic Press–Schechter-like formalism (specifically a hybrid of the Press–Schechter and Sheth–Tormen prescriptions, following the method of \red{Barkana \& Loeb}) that has been modified to include the effects of the local overdensity and streaming velocity on halo formation (Gessey-Jones 29–30). In essence, overdense regions and regions with low relative velocity form more halos, while underdense or high-$v_{\text{bc}}$ regions form fewer. This yields a spatially varying halo mass function across the grid (each cell gets a distribution or count of halos of different masses). The method is calibrated against N-body simulations and is more accurate at high redshift than a simple global Press–Schechter recipe (Gessey-Jones 29–30).

    \item Compute star formation rates (Pop III and Pop II): Given the halo population in a cell, 21cmSPACE next determines how many stars form in those halos. It uses a sub-grid star formation prescription adapted from \red{Magg et al. (2018)} to produce both Population III (metal-free) and Population II (normal metal-enriched) star formation in tandem (Gessey-Jones 30). In summary, when a halo first reaches the minimum mass to allow cooling and star formation, it is assumed to host a burst of Pop III star formation with an efficiency $f_{\textasteriskcentered,\text{III}}$ (a fraction of the halo’s baryons turn into Pop III stars). After this initial burst, the halo undergoes a recovery period of duration $t_{\text{delay}}$ (to account for feedback from Pop III supernovae blowing out gas). Once this delay time has passed, the halo can then sustain continuous Pop II star formation with efficiency $f_{\textasteriskcentered,\text{II}}$ going forward.Thus, each halo transitions from a Pop III-producing phase to a Pop II-producing phase. The parameters $f_{\textasteriskcentered,\text{III}}$, $f_{\textasteriskcentered,\text{II}}$, and $t_{\text{delay}}$ are inputs to the code that can be adjusted to explore different astrophysical scenarios (for instance, to test how a more or less efficient first-star formation would affect the 21-cm signal) (Gessey-Jones 30). In the implementation, a set of fitting formulas (based on high-resolution “merger-tree” simulations by \red{A-SLOTH}) is used to compute the star formation rate in each cell from the halo mass distribution, taking into account these efficiencies and the timing of halo formation (Gessey-Jones 30). The outcome of this step is a cell-averaged star formation rate density (SFRD) for Pop III and Pop II in every cell at the current time.

    \item Apply star formation feedback effects: As part of the star formation module, 21cmSPACE incorporates several key feedback mechanisms that can suppress star formation in low-mass halos. These include:
    \begin{itemize}
        \item Lyman-Werner (LW) feedback: Dissociating UV photons (in the LW band) can destroy molecular hydrogen, which metal-free (Pop III) star formation relies on. A strong LW background thus raises the minimum halo mass $M_{\text{crit}}$ needed for Pop III star formation.

        \item Streaming velocity feedback: A large baryon–DM streaming velocity (a remnant from pre-reionization structure formation) inhibits gas collapse into small halos. This effect also effectively increases the minimum halo mass for star formation in regions with high $v_{\text{bc}}$.

        \item Photoheating feedback: Once reionization begins, ionizing photons heat the IGM and can evaporate gas out of small halos, preventing them from forming stars. This mainly affects the later stages (Pop II in low-mass halos during reionization).
    \end{itemize}

    In 21cmSPACE, these feedbacks are represented by raising the local star-formation threshold mass $M_{\text{crit}}$ above a baseline value (set by a virial temperature or circular velocity criterion) in cells where the LW intensity, streaming velocity, or ionizing background are significant. The code includes formulas for how each feedback boosts $M_{\text{crit}}$ based on physical models or simulations (Gessey-Jones 30–31). By increasing $M_{\text{crit}}$, the star formation in that cell (especially Pop III) is suppressed accordingly. Additionally, 21cmSPACE offers an optional suppression of Pop II star formation efficiency in halos between the molecular-cooling threshold and the atomic-cooling threshold. This phenomenological tweak gradually reduces $f_{,\text{II}}$ in small halos that are just above $M_{\text{crit}}$, reflecting the idea that prior star formation in the halo can deplete or heat some of the gas (Gessey-Jones 31). When activated, this causes the Pop II star formation rate (SFR) to ramp up from zero at $M_{\text{crit}}$ to the nominal $f_{,\text{II}}$ by the time a halo reaches the atomic cooling mass scale. In the default scenarios studied, this extra suppression was not crucial and can be turned off. Another optional feature is to introduce stochasticity in star formation: instead of using the average SFR in each cell, the code can randomly sample the number of halos and their star formation outcomes (Poisson sampling) to mimic shot noise when halos are very few (Gessey-Jones 31). This stochastic mode captures additional fluctuations (important at very early times $z\gtrsim25$), but it makes the simulation nondeterministic and was turned off in favor of reproducibility and easier statistical analysis (Gessey-Jones 31). After applying all feedback effects, the result of steps 2–4 is a finalized Pop III and Pop II SFR for each cell at the current time step.

    \item Convert star formation to radiative emissivities: The newly computed star formation rates are then converted into emissivities of various radiation species. 21cmSPACE tracks several radiation fields that are critical for 21-cm physics:
    \begin{itemize}
        \item Lyman-series (Ly$\alpha$) photons: These are UV photons capable of scattering in the Ly$\alpha$ transition of hydrogen. They are responsible for the Wouthuysen–Field effect, which couples $T_S$ to the gas temperature.

        \item Lyman-Werner (LW) photons: UV photons in the 11.2–13.6 eV range that dissociate $\mathrm{H}_2$, as mentioned in feedback.

        \item Ionizing UV photons: Above 13.6 eV, these ionize hydrogen and drive reionization (discussed separately below).

        \item X-ray photons: These can travel far through the neutral IGM, heating it and also producing some secondary ionizations. \red{Fialkov et al.}

        \item Cosmic ray particles: High-energy particles (if included) that can propagate and heat the IGM.

        \item Radio emission: Any additional radio background (e.g., from early radio galaxies or dark matter decay) that would effectively raise $T_\gamma$.
    \end{itemize}

    For each of these, the code uses the star formation rate of Pop III and Pop II in a cell to calculate how many photons (or what luminosity) that cell produces. For example, Pop II star-forming halos are assumed to have an X-ray luminosity proportional to their star formation rate, 
    \begin{equation}
        L_X = (3\times10^{40} \text{erg s}^{-1} M_\odot^{-1}\text{yr}) f_{\text{x}} \text{SFR}
    \end{equation}
    
    where $f_X$ is the X-ray emission efficiency parameter (Gessey-Jones 33). A similar relation (with potentially a different $f_X$) can be used for Pop III halos. The spectra of emitted X-rays can be chosen (21cmSPACE allows using a template Pop II X-ray binary spectrum, a power-law, or a Pop III spectrum from detailed models) (Gessey-Jones 33). Lyman-band photons from stars are computed by assuming stellar population spectra: Pop II stars use a standard stellar population spectrum (e.g., from \red{Leitherer et al.}), while Pop III stellar spectra are derived from the Pop III initial mass function in the model (Gessey-Jones 32). In essence, the code multiplies the star formation rate by an appropriate luminosity or photon production yield to get the emissivity (photons per second per comoving volume) for each radiation field of interest in each cell.

    \item Propagate radiation fields through space: Once the emissivity (sources) are known, 21cmSPACE computes the radiation intensity filling each cell by propagating photons from all sources. A key simplification enabling fast computation is the use of a Fourier-space convolution (window function) method for radiative transfer (Gessey-Jones 32–33). Instead of tracing rays for millions of sources, the code uses precomputed spherical window functions that describe the average intensity profile around a source for each type of radiation. For photons that travel without absorption (like LW or Ly$\alpha$ beyond a certain wavelength), the window function is essentially a spherical top-hat: a photon travels until redshifted out of the band. For photons that undergo absorption (X-rays being absorbed by neutral gas, Ly$\alpha$ between Ly$\alpha$ and Ly$\beta$ undergoing scatterings), the window functions incorporate those effects (e.g. an exponential attenuation with distance for X-rays due to absorption cross-sections, or a diffusion kernel for Ly$\alpha$ scatterings). By convolving the emissivity field with these window functions, the code obtains the radiation energy density or intensity field in every cell. This convolution is done efficiently via Fast Fourier Transforms, treating the window function as a filter. The radiative transfer also accounts for redshift (cosmological) attenuation and light-cone effects (photon travel time across the simulation volume), ensuring that the finite speed of light and cosmic expansion are included (Gessey-Jones 32). The outcome is that for each cell we now have, for example, the local Ly$\alpha$ intensity $J_{\alpha}(\mathbf{x})$, the LW intensity, the X-ray flux spectrum $J_X(\mathbf{x},E)$, etc., at the current time. These fields are crucial for the next step. (Ionizing UV radiation is handled a bit differently via an excursion-set approach in the reionization step below, because ionizing photons create sharp ionized/non-ionized regions rather than a smoothly decaying intensity field.)

    \item Update gas temperature and ionization: Given the radiation fields and other local quantities, 21cmSPACE then advances the state of the intergalactic gas in each cell. This is done by solving a set of coupled differential equations that govern the heating/cooling of the gas and the growth of ionization. The code uses a numerical integrator (Runge–Kutta method) to solve these equations simultaneously at each time step (Gessey-Jones 35). One equation is the thermal evolution equation, which equates the change in gas kinetic temperature $T_K$ to various heating and cooling terms. Heating terms include X-ray heating (from the X-ray intensity computed earlier), Compton heating by CMB photons, Lyα heating, cosmic ray heating (if enabled), etc., all summed into a total heating rate per baryon for the cell (Gessey-Jones 35). Cooling terms include adiabatic cooling due to the expansion of the Universe and additional cooling from the increased number of particles if the gas is being ionized (since energy gets distributed into new free electrons). There is also a small heating term from structure formation (shock heating from collapse, which 21cmSPACE approximates based on the change in baryon density). At the same time, an equation for the evolution of the ionized fraction $x_e$ is solved. For ionization, 21cmSPACE adopts an excursion-set formalism to determine which regions become fully ionized by UV radiation (this handles reionization), coupled with a differential equation for partial ionization by X-rays and other minor sources (Gessey-Jones 35–36). In practice, the code first checks the collapse fraction (fraction of mass in halos) in a region against a threshold condition (governed by an ionizing efficiency parameter $\zeta$) using an excursion-set approach similar to Mesinger et al. – if a cell (or a larger region containing that cell, scanned over various scales) has $\zeta f_{\text{coll}}$ above a certain threshold, it is declared fully ionized by stellar UV photons. Those cells are set to $x_{\mathrm{HI}} = 0$. Cells that do not meet the criterion are not fully ionized by UV, but they can still have partial ionization from X-rays or cosmic rays. 21cmSPACE models those partially ionized cells as a two-phase medium: a fraction of the cell is treated as fully ionized (H II) and the remainder as mostly neutral with a small ionized fraction $x_{e,\text{oth}}$ due to X-rays etc. (Gessey-Jones 36–37). The effectively neutral fraction in a cell is then $x_{\mathrm{HI}} = 1 - \zeta f_{\text{coll}} - x_{e,\text{oth}}$ (ensuring that when UV ionizations $\zeta f_{\text{coll}}$ and other ionizations $x_{e,\text{oth}}$ sum to 1, the cell is fully ionized). The parameter $x_{e,\text{oth}}$ is computed from the X-ray ionization rate and any other non-UV contributions in the cell. By solving the temperature equation and updating ionization in this way, 21cmSPACE advances the spin temperature $T_S$ toward the kinetic temperature (accounting for Lyα coupling strength) and updates the neutral fraction, completing the evolution of the 21-cm brightness temperature field for that time step.

    \item Record outputs and reiteration: After updating the fields, any quantities that are needed later or for output are stored. For example, 21cmSPACE will save the current values of $T_\gamma$ (effective background radiation temperature), the star formation rates, the ionization fraction, etc., as well as intermediate fields it might need for post-processing (Gessey-Jones 37). The simulation then moves to the next redshift step and repeats the loop (steps 1–8) for that new time. This process continues until the final redshift ($z=6$) is reached.
\end{enumerate}

Through this loop, 21cmSPACE self-consistently evolves the cosmic gas and radiation fields from the initial conditions to the end of reionization. The result is a time-series of 3D fields (density, $T_K$, $T_S$, $x_{\mathrm{HI}}$, etc.) or equivalently the 21-cm brightness temperature field $\delta T_{21}(\mathbf{x}, z)$ at each redshift. These rich data contain the information needed to derive observable signatures of the 21-cm signal.

It should be noted that 21cmSPACE is optimized by precomputing any components of the calculations that do not depend on the particular astrophysical parameters of a run. For instance, the cosmological initial power spectra or the window functions for radiative transfer can be generated once and reused for many simulations. These are stored on disk and loaded as needed, saving runtime (Gessey-Jones 37). The code also allows for checkpointing: a simulation can be paused and resumed, which is useful if one wants to stop at an intermediate redshift and perhaps explore a branch of parameter space from there (Gessey-Jones 37). In normal operation, however, it runs straight through the loop described above.

\subsection{Post-Processing and Outputs}

After the main simulation loop concludes at $z=6$, 21cmSPACE enters a post-processing stage to produce high-level outputs for analysis. The two primary outputs are the global 21-cm signal and the 21-cm power spectrum, but the code can also derive other quantities of interest. First, the code compiles the results of all time steps to construct the full 21-cm brightness temperature field as a function of redshift. This involves combining the evolved spin temperature, density, and ionization fraction to calculate the differential brightness temperature $\delta T_{21}(\mathbf{x}, z)$ in each cell at each epoch. If any additional effects need to be applied (for example, line-of-sight peculiar velocity gradients cause what are known as redshift-space distortions in the observed 21-cm field), those are accounted for at this stage to ensure the final signal is in the form an observer would see (Gessey-Jones 37–38). In particular, 21cmSPACE can adjust the 21-cm field for redshift-space distortions before computing summary statistics. 

From this spatio-temporal data, the sky-averaged (global) 21-cm signal is obtained by averaging the 21-cm brightness over the entire simulation volume at each redshift. The result is a curve of the mean 21-cm brightness temperature as a function of redshift (or cosmic time). This global signal is a key target for experiments like EDGES and others, and it encapsulates the overall thermal and ionization history of the cosmic gas. 21cmSPACE’s global signal prediction includes the impact of all the modeled physics (e.g. it will show the deep absorption feature when the IGM is cold and strongly coupled to $T_S$, and then a rise toward emission as X-ray heating dominates, etc., followed by a decline to zero as reionization completes). The 21-cm power spectrum is computed by taking the Fourier transform of the 3D 21-cm fluctuation field at various redshifts and calculating the variance as a function of scale (wavenumber $k$). Specifically, 21cmSPACE will typically compute the dimensionless power spectrum $\Delta^2_{21}(k)$ from the simulation volume at a given redshift, which can be compared to interferometric observations (e.g. from HERA or the SKA). Because the simulation volume is finite, the code can reliably compute modes in a certain $k$ range (for the $128^3$ volume of side 384 cMpc, roughly $k \sim 0.05$ to $1.0~\text{cMpc}^{-1}$, covering the range where current 21-cm instruments are most sensitive) (Gessey-Jones 29). The inclusion of redshift-space distortion effects in the previous step ensures that the power spectrum is computed in a way that corresponds to observations (Gessey-Jones 37). The power spectrum captures the scale-dependent fluctuations induced by, for example, patchy reionization (which boosts large-scale power when large ionized bubbles form) or the clustering of early galaxies. In addition to these, 21cmSPACE can output other diagnostic information. For instance, it can evaluate the contribution of early sources to the present-day unresolved X-ray background in a manner consistent with the simulation (by integrating the X-ray emissivity over redshift) (Gessey-Jones 33–34). It also tracks the progress of reionization (e.g. the volume-averaged ionized fraction as a function of $z$) and could output the size distribution of ionized regions if needed for analysis. The code is flexible in storing any intermediate fields; in practice one could extract, say, the evolution of the LW intensity or the heating rate in the simulation for further study. However, the thesis work primarily focuses on the global signal and power spectrum as the summary observables. Finally, the results are typically interpreted in the context of astrophysical parameters. Because 21cmSPACE runs quickly compared to fully numerical simulations, it can be run for many different parameter combinations (e.g. different $f_{*,\text{III}}$, $f_X$, initial mass function assumptions, etc.) to see how the 21-cm outputs change. This makes it a powerful tool for comparing with data or forecasting the constraints that observations could place on the first stars. The comprehensive design of 21cmSPACE – incorporating time evolution, three-dimensional space, and detailed cosmological and astrophysical processes – enables it to serve as the engine behind theoretical studies of the 21-cm signal across cosmic epochs (Gessey-Jones 25–28). Its outputs, such as the global signal and power spectrum, are the bridge to observations, allowing researchers to test models of the first stars and galaxies against the forthcoming 21-cm measurements in a computationally tractable yet physically robust way.

%%

\section{Project Overview}

21cmSPACE, while being a powerful tool for simulation of the 21-cm signal, still holds some limitations. Particularly, as outlined in the \red{previous section}, despite there offering the option to include or exclude effects from a plethora of physical evolutionary processes, the instantiation of the simulation from its initial conditions remains confined to the use of its default Planck 2013 \red{best-guess} cosmology, and propagation thereof. This is an extremely strong assumption potentially affecting \red{most} of the physical processes throughout the evolution of the 21-cm signal through, as a prominent example, the growth rate of the universe. 

However, there is great value to be gained from the ability to evolve the 21-cm signal for variable cosmologies. As the amount of experimental data measuring various observable imprints of the 21-cm signal continues to increase, 

\newpage
\section{Theoretical Background}
\subsection{21cm Signal Fundamentals}
\subsection{Linear Cosmological Perturbations}
\subsection{Streaming Velocity Effect}
\subsection{Press-Schechter Halo Mass Function}
\subsection{Impact on First Stars and the 21-cm Signal}
\subsection{Thermal and Ionization History, with uncertainties}


\newpage
\section{Methodology}
\subsection{Baseline 21cmSPACE Operation}
\subsection{Initial Conditions Generation}
\subsection{Incorporation of New Halo Mass Functions}
\subsection{Consistency Checks}

\newpage
\section{Results}
\subsection{Simulation Suite}
\subsection{Global 21cm Signal}
\subsection{21cm Power Spectrum}
\subsection{Halo Mass Function Results}

\newpage
\section{Discussion}
\subsection{Implications for Cosmology with 21cm}
\subsection{Accuracy of Implementation}
\subsection{Comparison with Prior Work}
\subsection{Uncertainties and Error Bars}
\subsection{Relevance for Upcoming Experiments}

\newpage
\section{Conclusion and Future Work}
\subsection{Conclusions}
\subsection{Project Achievements}
\subsection{Future Work}

\section{Acknowledgements}



% \begin{acknowledgments}
% K.W. thanks...
% \end{acknowledgments}

\newpage
\nocite{*}
\printbibliography[title={References}]

\end{document}